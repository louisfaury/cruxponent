+++
author = "Louis Faury"
title = "A PPO Saga"
date = "2025-08-12"
+++

For better or for worse, proximal policy optimisation (PPO)
algorithms and its variants are dominating the RL landscape these days. 
This post aims at retracing their journey, from foundational concepts
to LLM-savy innovations. We will start this saga on the theoretical trail, 
which we will progressively abandon to pay closer attention to algorithmic aspects.

<!--more-->

{{< infoblock>}}
$\quad$ We rely on standard notations defined <a href="../mdp_basics" style="text-decoration:none; color:#0074aa;" ">here</a> – but expect some notational abuse to reduce clutter.
{{< /infoblock >}}


## Warm-up
### Preliminaries

Let $\mathcal{M} = (\mathcal{S}, \mathcal{A}, p, r)$ be the finite MDP of interest. 
The finiteness assumption is without loss of generality and adopted for the sake of simplicity; 
all arguments hold when replacing sums with integrals (up to a dominated convergence theorem application here and there).
We study $\mathcal{M}$ through a discounted objective:
$$
\tag{1}
v\_\pi := \mathbb{E}^\pi\_\nu\Big[\sum\_{t\geq 1}\lambda^{t-1}r(s\_t, a\_t)\Big]\\;,
$$
where $\nu$ is the starting state's distribution and $\pi$ is a stationary policy. 
The state-action values will be denoted as $q\_{\pi}$, and the advantage function $\text{a}\_\pi$.
We use state-indexed notation like $v\_\pi(s)$, when the starting state is conditioned to $s\_0=s$.
Throughout this post, we will alternate between using time-indexed definitions
and their equivalent state-occupancy counterpart.
For instance, by defining
$
d\_\pi(s) := \sum\_{t\geq 1} \lambda^{t-1} \mathbb{P}^\pi\_\nu(s\_t=s)
$
the discounted state occupancy measure,
we can re-write the value of $\pi$ as:

$$
\tag{2}
v\_\pi = \sum\_{s}d\_\pi(s)\sum\_{a} \pi(a\vert s)r(s, a)\\;.
$$


{{% toggle_block background-color="#FAD7A0" title="Proof" default-display="none"%}}
$$ 
\begin{aligned}
    v\_\pi  &= \mathbb{E}^\pi\_\nu\Big[\sum\_{t\geq 1}\lambda^{t-1}r(s\_t, a\_t)\Big]\\;, \\\
            &= \mathbb{E}^\pi\_\nu\Big[\sum\_{t\geq 1}\lambda^{t-1}\sum\_{s\in\mathcal{S}}\sum\_{a\in\mathcal{A}} \mathbf{1}[s\_t=s, a\_t=a]r(s, a)\Big]\\;, \\\
            &= \sum\_{t\geq 1}\lambda^{t-1}\sum\_{s\in\mathcal{S}}\sum\_{a\in\mathcal{A}} \mathbb{P}^\pi\left(s\_t=s, a\_t=a\right)r(s, a)\\;, \\\
            &= \sum\_{t\geq 1}\lambda^{t-1}\sum\_{s\in\mathcal{S}}\sum\_{a\in\mathcal{A}} \mathbb{P}^\pi\left(s\_t=s\right)\pi(a\vert s)r(s, a)\\;, &(\text{conditioning})\\\
            &= \sum\_{s\in\mathcal{S}}\left(\sum\_{t\geq 1}\lambda^{t-1}\mathbb{P}^\pi\left(s\_t=s\right)\right)\sum\_{a\in\mathcal{A}}\pi(a\vert s)r(s, a)\\;, &(\text{re-arranging})\\\
            &= \sum\_{s\in\mathcal{S}}d\_\pi(s)\sum\_{a\in\mathcal{A}}\pi(a\vert s)r(s, a)\\;.
\end{aligned}
$$
<div style="text-align: right"> $\blacksquare$&nbsp; </div>
{{% /toggle_block %}}

{{% toggle_block background-color="#CBE4FE" title="State occupancy probability measure" default-display="none"%}}
The measure $d\_\pi$ is not a probability measure since it does not sum to one. 
This would require the definition:
$$
d\_\pi(s) := (1-\lambda)\sum\_{t\geq 1} \lambda^{t-1} \mathbb{P}^\pi\_\nu(s\_t=s)\\;,
$$
which we will avoid for compactness.
On some occasions, though, we will pretend that $d\_\pi$ as defined in the main text to be a probability measure, so we can
write equations like (2) as expectations:
$$
v\_\pi = \mathbb{E}\_{s\sim d\_\pi, \\, a\sim\pi(\cdot\vert s)}\left[ r(s, a) \right]\\;,
$$
which is fine, since valid up to some multiplicative factors dependent on $\lambda$.
{{% /toggle_block %}}

### Policy Iteration
Let's start with a refresher on the policy iteration (PI) algorithm.
At every round $t$, PI starts by a _policy evaluation_ step.
It consist of computing $v\_{\pi\_t}(s)$ for all $s\in\mathcal{S}$
(in finite MDPs, this amounts to solving a linear equation)
which in turn directly gives us access the state-action values $q\_\pi(s, a)$.
PI then proceeds with a _policy improvement_ step, which creates a new deterministic policy by greedily following the best action
prescribed by the state-action values of the previous iterate:
$$
\tag{3}
\pi\_{t+1}(s) = \argmax\_{a\in\mathcal{A}} q\_{\pi\_t}(s, a)\\;.
$$

One particularly useful property of this algorithm is monotonic improvement.
Concretely, if $(\pi\_1, \ldots, \pi\_t)$ some sequence of stationary policies 
generated by PI, then it is guaranteed that 
$v\_{\pi\_1} \leq \ldots \leq v\_{\pi\_t}$. 
This property namely ensures that PI
converges in finite time in finite MDPs. 


{{< infoblock>}}
$\quad$ For a deeper description of the policy iteration algorithm, check out <a href="../mdp_basics_3" style="text-decoration:none; color:#0074aa;" ">this</a> post.
{{< /infoblock >}}

Policy Iteration is a foundational concept to countless RL algorithms – virtually all actor-critic methods.
Thanks to its monotonic improvements, it, however, enjoys something deep RL approaches envy: stability.

### Improving Policies

When parametrising policies via deep neural networks, 
one cannot directly perform the policy improvement step (3) 
and must rely on policy optimisation instead.
Under function approximation, that (3) holds for every state is unrealistic. 
Instead, we set out here for an alternative
way to capture monotonic improvement.
To this end, consider the following way to express 
the value difference between two stationary policies $\pi, \\, \pi^\prime$:

$$
\tag{4}
\begin{aligned}
    v\_{\color{black}\pi^\prime} - v\_{\color{black}\pi} &= \mathbb{E}^{\color{black}\pi^\prime}\_\nu\Big[\sum\_{t=1}\lambda^{t-1} \text{a}\_{\color{black}\pi}(s\_t, a\_t)\Big]\\;,\\\
                    &= \sum\_{s}d\_{\color{black}\pi^\prime}(s) \sum\_{a} {\color{black}\pi^\prime}(a\vert s) \\,\text{a}\_{\color{black}\pi}(s, a)\\;.
\end{aligned}
$$


(4) reveals that a sufficient condition for policy improvement (_i.e._ $v\_{\pi^\prime}\geq v\_\pi$) 
is that $\pi^\prime$, on average, selects actions that come 
with a positive advantage under $\pi$. In other words, that for every state $s\in\mathcal{S}$:
$$
\sum\_{a} \pi^\prime(a\vert s) \\,\text{a}\_\pi(s, a) \geq 0 \\;.
$$


{{% toggle_block background-color="#FAD7A0" title="Proof" default-display="none"%}}
Recall that for all $s, a\in\mathcal{S}\times\mathcal{A}$:
$$
\begin{aligned}
    \text{a}\_\pi(s, a)  &= q\_\pi(s, a) - v\_\pi(s)\\;, \\\
                    &= r(s, a) + \lambda \mathbb{E}\_{s^\prime \sim p(\cdot\vert s, a)}v\_\pi(s^\prime) - v\_\pi(s)\\;.
\end{aligned}
$$
Therefore:
$$ 
\begin{aligned}
\mathbb{E}^{\pi^\prime}\_\nu\Big[\sum\_{t=1}\lambda^{t-1} \text{a}\_\pi(s\_t, a\_t)\Big]  &= \mathbb{E}^{\pi^\prime}\_\nu\Big[\sum\_{t=1}\lambda^{t-1} r(s\_t, a\_t)\Big] + 
\mathbb{E}^{\pi^\prime}\_\nu\Big[\sum\_{t=1}\lambda^{t-1} \big(\mathbb{E}^{\pi^\prime}\_\nu\big[\lambda v\_{\pi}(s\_{t+1})\vert s\_t, a\_t)\big]-v\_\pi(s\_t)\big)\Big]\\;, \\\
&= v\_{\pi^\prime} + \mathbb{E}^{\pi^\prime}\_\nu\Big[\sum\_{t=1}\lambda^{t-1} \mathbb{E}^{\pi^\prime}\_\nu\big[\lambda v\_{\pi}(s\_{t+1}\vert s\_t, a\_t)\big]-\lambda^{t-1}v\_\pi(s\_t)\Big]\\;, &(\text{def.})\\\
&= v\_{\pi^\prime} + \mathbb{E}^{\pi^\prime}\_\nu\Big[\sum\_{t=1}\lambda^{t} v\_{\pi}(s\_{t+1})-\lambda^{t-1}v\_\pi(s\_t)\Big]\\;, &(\text{tower rule})\\\
&= v\_{\pi^\prime} - \lim\_{t\to\infty}\lambda^t \mathbb{E}^{\pi^\prime}\_\nu v\_\pi(s\_{t+1}) - v\_\pi\\;, &(\text{telescopic sum})\\\
&= v\_{\pi^\prime} - v\_\pi\\;.  &(\lambda<1)
\end{aligned}
$$
The rest of the proof simply requires indexing over state and actions as we
did for proving (2).
<div style="text-align: right"> $\blacksquare$&nbsp; </div>
{{% /toggle_block %}}
{{% toggle_block background-color="#CBE4FE" title="Some intuition" default-display="none"%}}
The above inequality can easily be rewritten as:
$$
\sum\_{a} \pi^\prime(a\vert s)q\_\pi(s, a) \geq \sum\_{a}\pi(s, a)q\_\pi(s, a)\\;.
$$

In other words, when sampling from $\pi^\prime$, 
we get, on average, higher values than when sampling from $\pi$. 
It is therefore not surprising that this translates into 
 $v\_{\pi^\prime}\geq v\_\pi$.

{{% /toggle_block %}}


It is tempting to use (4) as a basis for algorithmic design.
By maximising the r.h.s, we get a formula for monotonic improvements.
However, $d\_{\pi^\prime}$ depends on $\pi^\prime$ in intricate ways, 
not ideal for optimisation. Also, it would be more economical
to reuse whatever samples were drawn from $\pi$ to estimate its
advantage $\text{a}\_\pi$ during a policy evaluation step.
To meet this objective, we can 
decide to optimise the following alternative:
$$
\tag{5}
\ell ({\color{black}\pi^\prime}\vert {\color{black}\pi}) := \sum\_{s}d\_{\color{black}\pi}(s) \sum\_{a} {\color{black}\pi^\prime}(a\vert s) \\,\text{a}\_{\color{black}\pi}(s, a)\\;.
$$

Observe that we now sample under the state-occupancy distribution induced by $\pi$,
easing estimation and optimisation.
Optimising (5) sure is handy, but as we moved away from (4) there is no longer any improvement guarantee, at least
at a first glance.
In the following section, we cover a lower-bound which motivates
maximising $\ell(\pi^\prime\vert\pi)$ over some trust-region, which will give us back improvement guarantees.

## Performance Bound

<br>
{{< image src="/lowerbound.png" width="420px" align="center" caption="Fig1. Illustrating the lower-bound and the policy improvement protocol.">}}
<br>


### Statement

Define the maximum total-variation distance between $\pi$ and $\pi^\prime$ as
$
\\|\pi-\pi^\prime\\|\_\infty:=\max\_{s\in\mathcal{S}} d\_\text{TV}(\pi(\cdot\vert s),\pi^\prime(\cdot\vert s))\\;.
$
We will be shamelessly hiding constants and expressions into the symbol $\small\blacksquare$;
as in, not only universal constants but also functions of the discount, the policies, etc.
in an effort to reduce clutter and ease comprehension.
The following bound establishes that the performance gap between $\pi^\prime$ and $\pi$
can indeed be explained by $\ell(\pi^\prime\vert \pi)$, but gets looser as policies become too dissimilar – as measured by $\\|\pi-\pi^\prime\\|\_\infty$. 

{{< boxed title="Performance lower-bound" >}}
$\qquad\qquad\qquad\qquad\qquad\qquad \text{For any  stationary policies }\pi, \pi^\prime:\\$
$$
\tag{6}
v_{\pi^\prime} - v_\pi\geq \ell(\pi^\prime\vert \pi) - {\small\blacksquare} \,\|\pi-\pi^\prime\|_\infty^2\;.
$$
{{< /boxed >}}

This bound is _tight_ at $\pi^\prime=\pi$; the inequality is an equality
when policies match.
This gives us back guaranteed improvement: if $\pi^\star$
maximises the r.h.s of (6), then $v\_{\pi^\star}\geq v\_\pi$.
We illustrate this statement in Fig1 above.
This motivates a new policy
improvement step. With $\eta$ some pre-specified hyperparameter, at every round $t$ we can proceed by computing
a new, improved policy via:
$$
\tag{7}
\pi\_{t+1} \in\argmax\_{\pi^\prime} \left\\{\ell(\pi^\prime\vert\pi\_t) - \eta \\| \pi\_t- \pi^\prime\\|\_\infty^2\right\\}\\;.
$$

{{< infoblock>}}
$\quad$ This bound first appeared in {{< ref link="aoarl">}} [1]{{< /ref>}},  albeit in a less general form.
It was generalised in {{< ref link="safepo">}} [2]{{< /ref>}} and {{< ref link="trpo">}} [3]{{< /ref>}},
using different lines of proof. 
The proof of {{< ref link="trpo">}} [3]{{< /ref>}} uses a coupling argument, and is worth the look.
In the next section, we give a demonstration close to the one presented in {{< ref link="safepo">}} [2]{{< /ref>}}.
{{< /infoblock >}}

### Proof
Let's now move on proving (6). The first step is to relate $\ell(\pi^\prime\pi)$
to the state occupancy measures.

$$
\tag{8}
\ell(\pi^\prime\vert \pi) = v\_{\pi^\prime} - v\_\pi + \sum\_{s}(d\_\pi(s)-d\_{\pi^\prime}(s))\sum\_{a}(\pi^\prime(a\vert s)-\pi(a\vert s))\\,\text{a}\_\pi(s, a) \\;.
$$

{{% toggle_block background-color="#FAD7A0" title="Proof" default-display="none"%}}
Observe that:
$$
\begin{aligned}
\ell(\pi^\prime\vert \pi) &= \sum\_{s}d\_\pi(s)\sum\_{a}\pi^\prime(a\vert s)\\,\text{a}\_\pi(s, a) \\\;\\\
&= v\_{\pi^\prime} - v\_\pi + \sum\_{s}(d\_\pi(s)-d\_{\pi^\prime}(s))\sum\_{a}\pi^\prime(a\vert s)\\,\text{a}\_\pi(s, a) \\;, &(\text{by (4))} \\\
&= v\_{\pi^\prime} - v\_\pi + \sum\_{s}(d\_\pi(s)-d\_{\pi^\prime}(s))\sum\_{a}(\pi^\prime(a\vert s)-\pi(a\vert s))\\,\text{a}\_\pi(s, a) \\;.
\end{aligned}
$$
The last statement uses the fact that $\sum\_{a} \pi(a\vert s)\text{a}\_\pi(s, a)=0$. Indeed:
$$
\begin{aligned}
\sum\_{a} \pi(a\vert s)\text{a}\_\pi(s, a) &= \sum\_{a} \pi(a\vert s)q\_\pi(s, a) - v\_\pi(s) \sum\_{a} \pi(a\vert s)\\;, &(\text{def.}) \\\
&= \sum\_{a} \pi(a\vert s)q\_\pi(s, a) - v\_\pi(s) \\;, &(\sum\_{a} \pi(a\vert s)=1) \\\
&=  v\_\pi(s) -  v\_\pi(s) = 0\\;.
\end{aligned}
$$
<div style="text-align: right"> $\blacksquare$&nbsp; </div>
{{% /toggle_block %}}

This first result can be derived into the following lower-bound:
$$
\tag{9}
v\_{\pi^\prime} - v\_\pi \geq \ell(\pi^\prime\vert \pi) - \blacksquare\\|\pi-\pi^\prime\\|\_\infty\\|d\_\pi-d\_{\pi^\prime}\\|\_1 \\;.
$$


{{% toggle_block background-color="#FAD7A0" title="Proof" default-display="none"%}}
By (8):
$$
\begin{aligned}
v\_{\pi^\prime} - v\_\pi &\geq \ell(\pi^\prime\vert \pi) - \\Big\vert \sum\_{s}(d\_\pi(s)-d\_{\pi^\prime}(s))\sum\_{a}(\pi^\prime(a\vert s)-\pi(a\vert s))\\,\text{a}\_\pi(s, a)\\Big\vert\\;,\\\
&\geq \ell(\pi^\prime\vert \pi) - \max\_{a, s} \vert \text{a}\_\pi(s, a)\vert\\Big\vert \sum\_{s}(d\_\pi(s)-d\_{\pi^\prime}(s))\sum\_{a}(\pi^\prime(a\vert s)-\pi(a\vert s))\\Big\vert\\;,\\\
&\geq \ell(\pi^\prime\vert \pi) - \blacksquare\max\_{s}\\Big\vert\sum\_{a}\pi^\prime(a\vert s)-\pi(a\vert s)\\Big\vert\\Big\vert \sum\_{s}(d\_\pi(s)-d\_{\pi^\prime}(s))\\Big\vert\\;,\\\
&\geq \ell(\pi^\prime\vert \pi) - \blacksquare\\|\pi-\pi^\prime\\|\_\infty \sum\_{s}\vert d\_\pi(s)-d\_{\pi^\prime}(s)\vert \\;.
\end{aligned}
$$
The last statement uses the fact that $d\_\text{TV}(p, q) \propto \sum\_{x}\vert p(x) - q(x)\vert$.
<div style="text-align: right"> $\blacksquare$&nbsp; </div>
{{% /toggle_block %}}

To finish our bound, we must therefore
bound the $\ell\_1$ distance between two state occupancy measures. 
Somewhat naturally, the distance between $d\_\pi$ and $d\_{\pi^\prime}$
can be related by the distance between $\pi$ and ${\pi^\prime}$:
$$
\tag{10}
\\| d\_\pi - d\_{\pi^\prime}\\|\_1 \leq \blacksquare\\|\pi-\pi^\prime\\|\_\infty\\;.
$$

{{% toggle_block background-color="#FAD7A0" title="Proof" default-display="none"%}}
We will use vectorial notations for this proof – all definitions
can be found [here](../mdp_basics/#:~:text=Note-,Vectorial%20Notations,-We%20now%20introduce).
Matrix operator norms are denoted via $|||\cdot|||$.
Observe that:
$$
\begin{aligned}
\\| d\_\pi - d\_{\pi^\prime}\\|\_1 &= \sum\_{s}\Big\vert \sum\_{t\geq 1}\lambda^{t-1} \big(\mathbb{P}\_\pi(s\_t=s)-\mathbb{P}\_{\pi^\prime}(s\_t=s)\big)\Big\vert\\;,\\\
&\leq \sum\_{s}\sum\_{t\geq 1}\lambda^{t-1} \Big\vert\mathbb{P}\_\pi(s\_t=s)-\mathbb{P}\_{\pi^\prime}(s\_t=s)\Big\vert\\;,\\\
&\leq \sum\_{t\geq 1}\lambda^{t-1}\sum\_{s} \Big\vert\mathbb{P}\_\pi(s\_t=s)-\mathbb{P}\_{\pi^\prime}(s\_t=s)\Big\vert\\;,\\\
&= \sum\_{t\geq 1}\lambda^{t-1}\sum\_{s} \Big\vert\mathbf{\nu}^\top \mathbf{P}\_\pi^{t-1} \mathbf{s}-\mathbf{\nu}^\top \mathbf{P}\_{\pi^\prime}^{t-1} \mathbf{s}\Big\vert\\;,\\\
&= \sum\_{t\geq 1}\lambda^{t-1} \\|\mathbf{\nu}^\top \mathbf{P}\_\pi^{t-1} -\mathbf{\nu}^\top \mathbf{P}\_{\pi^\prime}^{t-1} \\|\_1\\;,\\\
&\leq \sum\_{t\geq 1}\lambda^{t-1} |||\mathbf{P}\_\pi^{t-1} -\mathbf{P}\_{\pi^\prime}^{t-1} |||\_\infty\\;, & (\text{def. of }|||\cdot|||\_\infty)\\\
&\leq \sum\_{t\geq 2}\lambda^{t-1} |||\mathbf{P}\_\pi^{t-1} -\mathbf{P}\_{\pi^\prime}^{t-1} |||\_\infty\\;,\\\
&\leq \sum\_{t\geq 2}\lambda^{t-1} |||\mathbf{P}\_\pi -\mathbf{P}\_{\pi^\prime} |||^{t-1}\_\infty\\;, & (\text{sub. mult. of }|||\cdot|||\_\infty)\\\
&= \frac{1}{1- \lambda|||\mathbf{P}\_\pi -\mathbf{P}\_{\pi^\prime} |||\_\infty}-1\\;, \\\
&\leq \frac{\lambda}{1- \lambda}|||\mathbf{P}\_\pi -\mathbf{P}\_{\pi^\prime} |||\_\infty\\;. \\\
\end{aligned}
$$
Now, we have to bound $|||\mathbf{P}\_\pi -\mathbf{P}\_{\pi^\prime} |||\_\infty$. Observe that:
$$
\begin{aligned}
|||\mathbf{P}\_\pi -\mathbf{P}\_{\pi^\prime} |||\_\infty &= \max\_{s}\sum\_{s^\prime} \Big\vert \mathbb{P}\_\pi(s^\prime\vert s)- \mathbb{P}\_{\pi^\prime}(s^\prime\vert s)\Big\vert\\;,\\\
&=  \max\_{s}\sum\_{s^\prime} \Big\vert \sum\_{a}p(s^\prime\vert s, a)(\pi(a\vert s)-\pi^\prime(a\vert s)\Big\vert\\;,\\\
&\leq  \max\_{s}\sum\_{s^\prime}  \sum\_{a}p(s^\prime\vert s, a)\Big\vert\pi(a\vert s)-\pi^\prime(a\vert s)\Big\vert\\;, &(p\geq 0)\\\
&=  \max\_{s}  \sum\_{a}\Big\vert\pi(a\vert s)-\pi^\prime(a\vert s)\Big\vert\sum\_{s^\prime}p(s^\prime\vert s, a)\\;,\\\
&=  \max\_{s}  \sum\_{a}\Big\vert\pi(a\vert s)-\pi^\prime(a\vert s)\Big\vert\\;, &(\sum\_{s^\prime}p(s^\prime\vert s, a)=1)\\\
&= 2\\| \pi - \pi^\prime\\|\_\infty\\;.
\end{aligned}
$$
The last statement uses the fact that $d\_\text{TV}(p, q) = 2\sum\_{x}\vert p(x) - q(x)\vert$.
This finishes the proof.
<div style="text-align: right"> $\blacksquare$&nbsp; </div>
{{% /toggle_block %}}

Putting everything together yields the announced bound.

{{% toggle_block background-color="#f7a3a3" title="But this proof is wrong!" default-display="none"%}}
If you read closely, you noticed that our proof for (10) is wrong.
The dependencies w.r.t $\pi$ and $\pi^\prime$ are correct, 
but our way to get there is not. 
In particular, a valid proof would yield a $(1-\gamma)^2$
dependency in the denominator. The result, as we mean to study it, is still correct, though!

The main mistake comes from the simplification of the geometric sum; there is indeed no
guarantee that $\\| \mathbf{P}\_\pi - \mathbf{P}\_{\pi^\prime}\\|\_1<1$.
Actually, $\\| \mathbf{P}\_\pi - \mathbf{P}\_{\pi^\prime}\\|\_1\leq 2\\|\pi-\pi^\prime\\|\_\infty$.
The result only holds for $\\|\pi-\pi^\prime\\|\_\infty$ _small enough_.

To see some actually correct proof, refer to the references handed out earlier.
My choice for giving this false proof is that, in my opinion, it captures all the necessary intuitions about why the result is right.
{{% /toggle_block %}}


## Algorithms


The last section motivated a new policy improvement protocol; 
inspired by (6), it prescribes that at some round $t$ we maximise
$\ell(\pi^\prime\vert \pi\_t) -\eta \||\pi^\prime-\pi\_t\\|\_\infty$
in order to produce the improved $\pi\_{t+1}$.
The parameter $\eta$ is a user-specified hyperparameter.
We are getting closer to practical algorithms, but we need some extra steps. 

Evaluating $\ell(\pi^\prime\vert \pi)$ involves a sum over all actions, 
which can itself be quite expensive. Ideally, we could write it as an expectation
under $\pi$ only; this would allow us to resort to sample-average estimators 
to form $\ell(\pi^\prime\vert \pi)$ (concretely, juste sample from $\pi$).
This is actually relatively easy, thanks to importance sampling:
$$
\ell\_\pi(\pi^\prime\vert \pi) = \mathbb{E}\_{s\sim d\_\pi}\mathbb{E}\_{a\sim\pi(s)}\Big[\frac{\pi^\prime(a\vert s)}{\pi(a\vert s)}\\, \text{a}\_\pi(s, a)\Big]\\;.
$$

{{% toggle_block background-color="#FAD7A0" title="Proof" default-display="none"%}}
This is a one liner;
$$
\begin{aligned}
\ell ({\color{black}\pi^\prime}\vert {\color{black}\pi}) &:= \sum\_{s}d\_{\color{black}\pi}(s) \sum\_{a} {\color{black}\pi^\prime}(a\vert s) \\,\text{a}\_{\color{black}\pi}(s, a)\\;,\\\
&= \sum\_{s}d\_{\color{black}\pi}(s) \sum\_{a} \pi(a\vert s)\frac{\pi^\prime(a\vert s)}{\pi(a\vert s)} \\,\text{a}\_{\color{black}\pi}(s, a)\\;.
\end{aligned}
$$
<div style="text-align: right"> $\blacksquare$&nbsp; </div>
{{% /toggle_block %}}

The maximum total variation $\||\pi^\prime-\pi\_t\\|\_\infty$ is also tricky to compute 
without enumerating all states. It can be replaced by an expected total variation under $d\_\pi$;
that is $\mathbb{E}\_{s\sim d\_\pi}[d\_\text{TV}(\pi(s), \pi^\prime(s))]$, which can directly
be estimated by sampling from trajectories generated by $\pi$. Turns out, this is not just
a convenient approximation; it is also motivated by a variant of (6) that includes the expected total
variation distance {{< ref link="cpo">}} [4]{{< /ref>}}.
That's it, we have a practical algorithm! It repeatedly goes through the improvement step:
$$
\tag{11}
\pi\_{t+1} \in \argmax\_{\pi} \mathbb{E}\_{s\sim d\_{\pi\_t}}\Big[\mathbb{E}\_{a\sim\pi\_t(s)}\Big[\frac{\pi(a\vert s)}{\pi\_t(a\vert s)}\\, \text{a}\_{\pi\_t}(s, a)\Big]- \eta d\_\text{TV}(\pi(s), \pi\_t(s))\Big] 
$$

which is amenable to stochastic optimisation. The protocol is simple;
generate data from $\pi\_t$, use this to build a sample-average estimators for the 
objective in (11), compute $\pi\_{t+1}$, repeat. Several algorithms 
spawn from this approach; they differ mostly in how they control the distance between two iterates 
$\pi\_{t}$ and $\pi\_{t+1}$.

{{< infoblock>}}
$\quad$ Advantage estimation is an orthogonal topic that will be
covered in another post; we will discuss it by the end of this post, but for now we assume perfect access to the advantages $\text{a}_\pi$.
{{< /infoblock >}}


### TRPO
The total variation distance is not necessarily great to handle in numerical optimisation
schemes; beyond not being smooth, it also does not always have a closed form – even for standard 
distributions. The authors of the TRPO paper {{< ref link="trpo">}} [3]{{< /ref>}} replaces it by the 
KL divergence. Observe that this can also be justified by a lower-bound, thanks to
[Pinsker's inequality](https://en.wikipedia.org/wiki/Pinsker%27s_inequality).
TRPO also turns the unconstrained (11) into a constrained program:
$$
\tag{12}
\begin{aligned}
\pi\_{t+1} \in &\argmax\_{\pi}&  &\mathbb{E}\_{s\sim d\_{\pi\_t}}\mathbb{E}\_{a\sim\pi\_t(s)}\Big[\frac{\pi(a\vert s)}{\pi\_t(a\vert s)}\\, \text{a}\_{\pi\_t}(s, a)\Big]\\;, \\\
& & &\text{s.t }\mathbb{E}\_{s\sim d\_{\pi\_t}}\Big[d\_\text{KL}(\pi(s), \pi\_t(s))\Big] \leq \delta\\;.
\end{aligned}
$$
where $\delta$ is an hyperparameter. A search direction is produced by a second-order method; the inverse Hessian of the constraints 
(a.k.a, here, the Fischer information matrix) is multiplied by the objective's gradient. A line search
is then performed along the search direction to maximise the objective under the constraint.



### PPO
TRPO's implementation does not stray too far from our principled arguments. However, solving
(12) is relatively expensive as it goes through a second order method.
The authors of PPO {{< ref link="ppo">}} [5]{{< /ref>}} introduced a rough but effective approximation.
To understand it, observe that the expected total variation can be written as:
$$
\mathbb{E}\_{s\sim d\_{\pi\_t}}\Big[d\_\text{TV}(\pi(s), \pi\_t(s))\Big] = \mathbb{E}\_{s\sim d\_{\pi\_t}}\mathbb{E}\_{a\sim\pi(s)}\Big[\Big\vert \frac{\pi(a\vert s)}{\pi\_t(a\vert s)}-1\Big\vert\Big]\\;.
$$

{{% toggle_block background-color="#FAD7A0" title="Proof" default-display="none"%}}
Let $p, q$ two probability mass functions over some discrete $\mathcal{X}$.
Assume that $q(x)>0$ for all $x\in\mathcal{X}$. Then;
$$
\begin{aligned}
d\_\text{TV}(p, q) &= \frac{1}{2} \sum\_{x\in\mathcal{X}} \vert p(x) - q(x)\vert\\;,\\\
&=\frac{1}{2} \sum\_{x\in\mathcal{X}} q(x)\vert p(x)/q(x) - 1\vert\\;,\\\
&= \mathbb{E}\_q[\vert p(x)/q(x) - 1\vert] \\;.
\end{aligned}
$$
<div style="text-align: right"> $\blacksquare$&nbsp; </div>
{{% /toggle_block %}}
In other words, the (expected) total-variation will remain small as long as the ratio $\pi^\prime(a\vert s)/\pi(a\vert s)$ remains small, 
for state-action pairs generated by $\pi$. This motivated an improvement step which prevented
the importance weights to stray too far from 1. Concretely, denote $\vert \cdot\vert\_a^b$ the clipping operator:
for any
$a, b, x\in\mathbb{R}$ we have $|x|\_a^b = \min(b, \max(a, x))$.
Then, for $\varepsilon$ a small (close to 0) hyperparameter, the PPO objective writes:
$$
\tag{13}
\pi\_{t+1} \in \argmax\_{\pi}\mathbb{E}\_{s\sim d\_{\pi\_t}}\mathbb{E}\_{a\sim\pi\_t(s)}\Big[\Big\vert\frac{\pi(a\vert s)}{\pi\_t(a\vert s)}\Big\vert_{1-\varepsilon}^{1+\varepsilon}\\, \text{a}\_{\pi\_t}(s, a)\Big]\\;.
$$
It can be optimised using first-order stochastic optimisation methods, at much lower cost than the TRPO objective.
Modern implementations of PPO combine several other tricks or code-level optimisation; the main idea remains the same,
and powers many recent RL successes.

### SPO
One advantage that is often claimed for the PPO objective (13) is its simplicity. 
It does have a few holes, however. It was first demonstrated by
{{< ref link="trppo">}} [6]{{< /ref>}} that importance weight clipping
is a poor proxy for controlling the size of the policy update. 
There is a simple reason; assume that for some given state-action
couple $(\tilde{s}, \tilde{a})$ we have $\pi(\tilde s, \tilde a) > (1+\varepsilon)\pi\_t(\tilde s, \tilde a)$.
For such a point, the gradients of (13) w.r.t $\pi$ are zero. In other words, 
there is no mechanism for
keeping the propensity weights at $(\tilde{s}, \tilde{a})$ close to the boundary–at least not while using first-order methods.
The influence of other state-action pairs can push them arbitrarily far from it. 

<br>
{{< image src="/boundary.png" width="220px" align="center" caption="Fig2. From Fig3 of [7]. Disks represent importance weights, laying inside and outside the boundary. Arrows represent update directions or gradients; while weights that lie inside the boundary are pushed towards it, there is no mechanisms to prevent weights that lie outside of the boundary to stray even further from it.">}}
<br>

Below, we denote the importance sampling weights as
$\omega\_\pi(s, a) := \pi(s, a) / \pi\_{t}(s, a)$.
The authors of SPO {{< ref link="spo">}} [7]{{< /ref>}} proposed a simple fix to the
original PPO idea, that offer a better control on the size of the policy update.
Concretely, they propose to undergo the following policy improvement step:
$$
\tag{14}
\pi\_{t+1} \in \argmax \_\pi \mathbb{E}\_{s\sim d\_{\pi\_t}}\mathbb{E}\_{a\sim\pi\_t(s)} \Big[\omega\_\pi(s, a) \text{a}\_\pi(s, a) - \frac{\vert \text{a}\_\pi(s, a)\vert}{2\varepsilon}(\omega\_\pi(s, a)-1)^2\Big]\\;.
$$
Perhaps the cleanest intuition as to why this constitutes a promising update rule is the following; observe
that for a single action-pair (s, a), the updated importance weight will match the boundary exactly –
and without going through any zero-gradient phase, whatever the weight's initial value:
$$
\min\_\omega\big\\{\omega\_\pi(s, a) \text{a}\_\pi(s, a) - \frac{\vert \text{a}\_\pi(s, a)\vert}{2\varepsilon}(\omega\_\pi(s, a)-1)^2\big\\} = 1 + \text{sign}({\text{a}\_\pi(s, a)})\varepsilon\\;.
$$

### Leftovers
Today, PPO is the most widespread implementation of (11).
Modern implementations, especially in the world of LLMs, also add a KL-penalty to 
further constraint the size of the policy update or avoid large deviation from an initial model.
From a policy improvement perspective, that's roughly how far it goes.

We have left the topic of advantage estimation on the side until now.
Most practical implementations rely on Generalised Advantage Estimation (GAE)  {{< ref link="gae">}} [8]{{< /ref>}}.
It has close ties with TD($\lambda$), and allows to strike a fine bias-variance trade-off in advantage estimation.
Roughly, it relies on maintaining a model for the state value function $v\_\pi$, and computes each advantage estimators
$
    \hat{\text{a}}\_\pi^k(s\_t, a\_t) := \sum\_{i=0}^{k-1} \lambda^i r(s\_{t+i}, a\_{t+i}) + \lambda^k v\_\pi(s\_{t+k}) \\;.
$
A final advantage estimate is obtained via an (exponential) weighted sum of said estimates.

Maintaining a state value function can be expensive, and in applications like LLM alignment, is not justified.
LLM trainers seem to have fallen back to pure Monte-Carlo for advantage estimation.
For instance, the authors of {{< ref link="grpo">}} [9]{{< /ref>}} rely on sampling several
trials from the same state to form advantages for each action.


## References

<div id="aoarl"></div>
[1] Approximately Optimal Approximate Reinforcement Learning. Kakade and Langord, 2002.

<div id="safepo"></div>
[2] Safe Policy Iteration, Pirotta & al, 2013.

<div id="trpo"></div>
[3] Trust Region Policy Optimization. Schulman & al, 2015.

<div id="cpo"></div>
[4] Constrained Policy Optimization. Achiam & al, 2017.

<div id="ppo"></div>
[5] Proximal Policy Optimization Algorithms. Schulman & al, 2017.

<div id="trppo"></div>
[6] Truly Proximal Policy Optimization. Wang & al, 2019.

<div id="spo"></div>
[7] Simple Policy Optimization. Xie & al, 2025.

<div id="gae"></div>
[8] High-Dimensional Continuous Control Using Generalized Advantage Estimation. Schulman & al, 2016.

<div id="grpo"></div>
[9] Pushing the Limits of Mathematical Reasoning in Open Language Models. Shao & al, 2024.
<div id="blank"></div>

